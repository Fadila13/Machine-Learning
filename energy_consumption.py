# -*- coding: utf-8 -*-
"""Energy_consumption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10O9rVWycuplHWlRU5MoN2ntzEf1-1aai

# **Data Loading**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# load the dataset
url = '/content/drive/MyDrive/Data-full-Consumtion-Energi.csv'
consumption = pd.read_csv(url)
consumption

consumption.shape

"""Output kode di atas memberikan informasi sebagai berikut:

Ada 1000 baris records dalam dataset.
Terdapat 10 kolom yaitu: Temperature,	Humidity,	SquareFootage,	Occupancy,	HVACUsage,	LightingUsage,	RenewableEnergy,	DayOfWeek,	Holiday, dan	EnergyConsumption

# **Exploratory Data Analysis**

## **Deskripsi Variabel**

* Temperature	      : Suhu
* Humidity	        : Kelembaban
* SquareFootage	    : Rekaman Persegi
* Occupancy	        : Hunian
* HVACUsage	        : Penggunaan HVACU
* LightingUsage	    : Penggunaan pencahayaan
* RenewableEnergy	  : Energi terbarukan
* DayOfWeek	        : Hari dalam seminggu
* Holiday	          : Hari libur
* EnergyConsumption : Konsumsi energi
"""

consumption.info()

"""Dari output terlihat bahwa:

Terdapat 4 kolom dengan tipe object, yaitu: HVACUsage, LightingUsage, DayOfWeek, Holiday. Kolom ini merupakan categorical features (fitur non-numerik).
Terdapat 5 kolom numerik dengan tipe data float64 yaitu: Temperature, Humidity, SquareFootage, RenewableEnergy dan EnergyConsumption.
Terdapat 1 kolom numerik dengan tipe data int64, yaitu: Occupancy.
"""

consumption.describe()

"""Fungsi describe() memberikan informasi statistik pada masing-masing kolom, antara lain:

Count adalah jumlah sampel pada data.
Mean adalah nilai rata-rata.
Std adalah standar deviasi.
Min yaitu nilai minimum setiap kolom.
25% adalah kuartil pertama. Kuartil adalah nilai yang menandai batas interval dalam empat bagian sebaran yang sama.
50% adalah kuartil kedua, atau biasa juga disebut median (nilai tengah).
75% adalah kuartil ketiga.
Max adalah nilai maksimum.

# **Data Assesing**

## **Data Duplikat**
"""

consumption.duplicated().sum()

"""## **Missing Value**"""

consumption.isna().sum()

"""# **Data Cleaning**

## **Menangani Outliers**

Beberapa pengamatan dalam satu set data kadang berada di luar lingkungan pengamatan lainnya. Pengamatan seperti itu disebut outlier.

Ada beberapa teknik untuk menangani outliers, antara lain:

* Hypothesis Testing
* Z-score method
* IQR Method

Pada kasus ini, Anda akan mendeteksi outliers dengan teknik visualisasi data (boxplot). Kemudian, Anda akan menangani outliers dengan teknik IQR method

IQR = Inter Quartile Range

IQR = Q3 - Q1.
"""

#Seltman dalam “Experimental Design and Analysis” [24] menyatakan bahwa outliers yang diidentifikasi oleh boxplot (disebut juga “boxplot outliers”) didefinisikan sebagai data yang nilainya 1.5 QR di atas Q3 atau 1.5 QR di bawah Q1.
#Hal pertama yang perlu Anda lakukan adalah membuat batas bawah dan batas atas. Untuk membuat batas bawah, kurangi Q1 dengan 1,5 * IQR. Kemudian, untuk membuat batas atas, tambahkan 1.5 * IQR dengan Q3.

consumption1=consumption.select_dtypes(exclude=['object'])
for column in consumption1:
        plt.figure()
        sns.boxplot(data=consumption1, x=column)

Q1 = consumption.quantile(0.25)
Q3 = consumption.quantile(0.75)
IQR=Q3-Q1

housing = consumption[~((consumption<(Q1-1.5*IQR))|(consumption>(Q3+1.5*IQR))).any(axis=1)]

"""Cek ukuran dataset setelah drop outliers dengan housing.shape"""

consumption.shape

"""Dataset Anda sekarang telah bersih dan memiliki 100 sampel

# **Data Analysis**

Selanjutnya, lakukan proses analisis data dengan teknik Univariate EDA. Pertama, Anda bagi fitur pada dataset menjadi dua bagian, yaitu numerical features dan categorical features.
"""

categorical_features = ['HVACUsage', 'LightingUsage', 'DayOfWeek', 'Holiday']
numerical_features = ['Timestamp', 'Temperatur',	'Humidity',	'SquareFootage',	'Occupancy',	'RenewableEnergy',	'EnergyConsumption']

"""## **Univariate Analysis**

Univariate visualization merupakan bentuk visualisasi data yang hanya merepresentasikan informasi yang terdapat pada satu variabel. Jenis visualisasi ini umumnya digunakan untuk memberikan gambaran terkait distribusi sebuah variabel dalam suatu dataset.

a. Data Kategori
"""

feature = categorical_features[0]
count = consumption[feature].value_counts()
percent = 100*consumption[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

feature = categorical_features[1]
count = consumption[feature].value_counts()
percent = 100*consumption[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

feature = categorical_features[2]
count = consumption[feature].value_counts()
percent = 100*consumption[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

feature = categorical_features[3]
count = consumption[feature].value_counts()
percent = 100*consumption[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""b. Data Numerik"""

consumption.hist(bins=50, figsize=(20,15))
plt.show()

"""Dari histogram "ConsumptionEnergy", diperoleh beberapa informasi, antara lain:

* Peningkatan Konsumsi Energi sebanding dengan penurunan jumlah sampel. Hal ini dapat terlihat jelas dari histogram "ConsumptionEnergy" yang grafiknya mengalami penurunan seiring dengan semakin banyaknya jumlah sampel (sumbu x).

* Distribusi konsumsi energi miring ke kanan (right-skewed). Hal ini akan berimplikasi pada model.

## **Multivariate Analysis**

Multivariate visualization merupakan jenis visualisasi data untuk menggambarkan informasi yang terdapat dalam lebih dari dua variabel. Jenis visualisasi ini digunakan untuk merepresentasikan hubungan dan pola yang terdapat dalam multidimensional data.

Pada tahap ini,cek rata-rata "ConsumptionEnergy" terhadap masing-masing fitur untuk mengetahui pengaruh fitur kategori terhadap "ConsumptionEnergy".

a. Data Kategori
"""

cat_features = consumption.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y='EnergyConsumption', kind="bar", dodge=False, height = 4, aspect = 3,  data=consumption, palette="Set3")
  plt.title("Rata-rata 'EnergyConsumption' relatif terhadap - {}".format(col))

"""Dengan mengamati rata-rata 'EnergyConsumption' relatif terhadap fitur kategori di atas, diperoleh insight sebagai berikut:

1. Pada fitur 'HVACUsage'
* Rata-rata 'EnergyConsumption' hanya terdapat 2 bervariasi. Rentangnya berada antara lebih dari 7 hingga kurang dari 8.
* Nilai 'EnergyConsumption'  pada nilai 'HVACUsage' cukup setara karena hanya ada dua pilihan yaitu On dan Off dan nilai nya tidak terlihat perbedaannya secara signifikan. Namun, fitur 'HVACUsage' memiliki pengaruh yang signifikan terhadap rata-rata 'EnergyConsumption'.
2. Pada Fitur 'LightingUsage'
* Rata-rata 'EnergyConsumption' hanya terdapat 2 bervariasi. Rentangnya berada antara lebih dari 7 hingga kurang dari 8.
* Nilai 'EnergyConsumption'  pada nilai 'LightingUsage' cukup setara karena hanya ada dua pilihan yaitu On dan Off dan nilai nya tidak terlihat perbedaannya secara signifikan. Namun, fitur 'LightingUsage' memiliki pengaruh yang signifikan terhadap rata-rata 'EnergyConsumption'.
3. Pada Fitur 'DayOfWeek'
* Rata-rata 'EnergyConsumption' cukup bervariasi senanyak Jumlah Hari. Rentangnya juga berada antara lebih dari 7 hingga kurang dari 8.
* Nilai 'EnergyConsumption' tertinggi berada pada nilai 'DayOfWeek' yaitu 'Friday' dan nilai 'EnergyConsumption' terendah berada pada nilai 'DayOfWeek' yaitu 'Tuesday'. Sehingga, fitur 'DayOfWeek' juga memiliki pengaruh yang signifikan terhadap rata-rata 'EnergyConsumption'.
4. Pada Fitur 'Holiday'
* Rata-rata 'EnergyConsumption' hanya terdapat 2 bervariasi. Rentangnya berada antara lebih dari 7 hingga kurang dari 8.
* Nilai 'EnergyConsumption'  pada nilai 'LightingUsage' cukup setara karena hanya ada dua pilihan yaitu Yes dan No dan nilai nya tidak terlihat perbedaannya secara signifikan. Namun, fitur 'LightingUsage' juga memiliki pengaruh yang signifikan terhadap rata-rata 'EnergyConsumption'.

**Kesimpulan akhir, fitur kategori memiliki pengaruh terhadap 'EnergyConsumption'.**

b. Data Numerik

Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
"""

sns.pairplot(consumption, diag_kind = 'kde')

"""Fungsi pairplot dari library seaborn menunjukkan relasi pasangan dalam dataset. Dari grafik, terlihat plot relasi masing-masing fitur numerik pada dataset. Pada pola sebaran data grafik pairplot sebelumnya, terlihat bahwa 'Temperature', 'Humidity', dan 'SquareFootage' memiliki korelasi dengan fitur 'EnergyConsumption'. Sedangkan kedua fitur lainnya terlihat memiliki korelasi yang lemah karena sebarannya tidak membentuk pola

Koefisien korelasi berkisar antara -1 dan +1. Ia mengukur kekuatan hubungan antara dua variabel serta arahnya (positif atau negatif). Mengenai kekuatan hubungan antar variabel, semakin dekat nilainya ke 1 atau -1, korelasinya semakin kuat. Sedangkan, semakin dekat nilainya ke 0, korelasinya semakin lemah

Untuk mengevaluasi skor korelasinya, gunakan fungsi corr().
"""

plt.figure(figsize=(10, 8))
correlation_matrix = consumption.corr().round(2)
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Jika diamati, fitur 'Occupancy' memiliki skor korelasi yang cukup besar (0.04) dengan fitur target 'EnergyConsumption'. Artinya, fitur 'EnergyConsumption' berkorelasi cukup tinggi dengan kedua fitur tersebut. Sementara itu, fitur lainnya memiliki korelasi negatif sehingga, fitur tersebut dapat di-drop."""

consumption.drop(['Temperature', 'Humidity','Occupancy'], inplace=True, axis=1)
consumption.head()

"""# **Data Preparation**

Data Peparation merupakan tahapan penting dalam proses pengembangan model machine learning. Pada tahap ini lakukan proses transformasi pada data sehingga menjadi bentuk yang cocok untuk proses pemodelan. Ada beberapa tahapan yang umum dilakukan pada data preparation, antara lain, seleksi fitur, transformasi data, feature engineering, dan dimensionality reduction.

Pada bagian ini lakukan empat tahap persiapan data, yaitu:

* Encoding fitur kategori.
* Reduksi dimensi dengan Principal Component Analysis (PCA).
* Pembagian dataset dengan fungsi train_test_split dari library sklearn.
* Standarisasi.

## **Encoding**

Untuk melakukan proses encoding fitur kategori, salah satu teknik yang umum dilakukan adalah teknik one-hot-encoding. Lakukan proses encoding ini dengan fitur get_dummies.
"""

from sklearn.preprocessing import  OneHotEncoder
consumption = pd.concat([consumption, pd.get_dummies(consumption['LightingUsage'], prefix='LightingUsage')],axis=1)
consumption.drop(['LightingUsage'], axis=1, inplace=True)
consumption.head()

from sklearn.preprocessing import  OneHotEncoder
consumption = pd.concat([consumption, pd.get_dummies(consumption['DayOfWeek'], prefix='DayOfWeek')],axis=1)
consumption.drop(['DayOfWeek'], axis=1, inplace=True)
consumption.head()

from sklearn.preprocessing import  OneHotEncoder
consumption = pd.concat([consumption, pd.get_dummies(consumption['Holiday'], prefix='Holiday')],axis=1)
consumption.drop(['Holiday'], axis=1, inplace=True)
consumption.head()

from sklearn.preprocessing import  OneHotEncoder
consumption = pd.concat([consumption, pd.get_dummies(consumption['HVACUsage'], prefix='HVACUsage')],axis=1)
consumption.drop(['HVACUsage'], axis=1, inplace=True)
consumption.head()

"""## **Reduksi Dimensi dengan PCA**

Teknik reduksi (pengurangan) dimensi adalah prosedur yang mengurangi jumlah fitur dengan tetap mempertahankan informasi pada data. Teknik pengurangan dimensi yang paling populer adalah Principal Component Analysis atau disingkat menjadi PCA. Ia adalah teknik untuk mereduksi dimensi, mengekstraksi fitur, dan mentransformasi data dari “n-dimensional space” ke dalam sistem berkoordinat baru dengan dimensi m, di mana m lebih kecil dari n.
"""

sns.pairplot(consumption[['Temperature', 'Humidity', 'Occupancy', 'RenewableEnergy']], plot_kws={"s": 4});

"""Berdasarkan hasil visualisasi dapat diketahui yang memiliki hubungan antar fitur hanya tiga yaitu 'Temperature', 'Humidity', 'RenewableEnergy'."""

sns.pairplot(consumption[['Temperature', 'Humidity', 'RenewableEnergy']], plot_kws={"s": 3});

"""Selanjutnya, reduksi 3 fitur ini dengan PCA"""

from sklearn.decomposition import PCA

pca = PCA(n_components=3, random_state=123)
pca.fit(consumption[['Temperature', 'Humidity', 'RenewableEnergy']])
princ_comp = pca.transform(consumption[['Temperature', 'Humidity', 'RenewableEnergy']])

"""Setelah menerapkan class PCA, cek proporsi informasi dari ketiga komponen PCs tadi."""

pca.explained_variance_ratio_.round(3)

"""Dari output di atas 60.2% informasi pada ketiga fitur 'Temperature', 'Humidity', 'RenewableEnergy' terdapat pada PC pertama. Sedangkan sisanya, sebesar 20.6% dan 19.2% terdapat pada PC kedua dan ketiga.

Berdasarkan hasil ini, reduksi fitur (dimensi) dan hanya mempertahankan PC (komponen) pertama saja. PC pertama ini akan menjadi fitur 'energy properties' menggantikan ketiga fitur lainnya ('Temperature', 'Humidity', 'RenewableEnergy'). Beri nama fitur ini 'energy properties'
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=1, random_state=123)
pca.fit(consumption[['Temperature', 'Humidity', 'RenewableEnergy']])
consumption['energy properties'] = pca.transform(consumption.loc[:, ('Temperature', 'Humidity', 'RenewableEnergy')]).flatten()
consumption.drop(['Temperature', 'Humidity', 'RenewableEnergy'], axis=1, inplace=True)
consumption.head()

"""## **Train-Test-Split**

Membagi dataset menjadi data latih (train) dan data uji (test) merupakan hal yang harus diakukan sebelum membuat model. Hal ini diperlukan untuk menguji seberapa baik generalisasi model terhadap data baru.

Pada model ini, proporsi pembagian sebesar 90:10 dengan fungsi train_test_split dari sklearn.
"""

from sklearn.model_selection import train_test_split

X = consumption.drop(["EnergyConsumption", "HVACUsage", "LightingUsage", "DayOfWeek", "Holiday"],axis =1)
y = consumption["EnergyConsumption"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""## **Standarisasi**

Algoritma machine learning memiliki performa lebih baik dan lebih cepat ketika dimodelkan pada data dengan yang mendekati distribusi normal. Scaling dan standarisasi merupakan metode yang dapat dilakukan

Untuk fitur numerik, tidak dilakukan proses transformasi dengan one-hot-encoding seperti pada fitur kategori. Yang digunakan untuk standarisasi adalah StandarScaler.

StandardScaler merupakan proses standarisasi fitur dengan mengurangkan mean kemudian membaginya dengan standar deviasi untuk menggeser distribusi. StandardScaler menghasilkan distribusi dengan standar deviasi sama dengan 1 dan mean sama dengan 0.
"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['SquareFootage',	'Occupancy', 'energy properties']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""Seperti yang telah disebutkan sebelumnya, proses standarisasi mengubah nilai rata-rata (mean) menjadi 0 dan nilai standar deviasi menjadi 1. Untuk itu perlu dicek nilai mean dan standar deviasi pada setelah proses standarisasi."""

X_train[numerical_features].describe().round(4)

"""Berdasarkan tabel di atas, sekarang nilai mean = 0 dan standar deviasi = 1.

# **Model Development**

Model development adalah tahapan dimana digunakan algoritma machine learning untuk menjawab problem statement dari tahap business understanding Pada tahap ini, dibuat model machine learning dengan tiga algoritma. Kemudian, evaluasi performa masing-masing algoritma dan pilih algoritma mana yang memberikan hasil prediksi terbaik. Ketiga algoritma yang akan digunakan, antara lain:

1. Regresi Linier
2. Regresi Ridge
3. Random Forest
4. Random Forest dengan Tuning GridSearchCV
"""

#Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['LinearRegression', 'RidgeRegression', 'RandomForest', 'RandomForest_GridSearchCV'])

"""## **1. Regresi Linear**

Regresi linear adalah teknik analisis data yang memprediksi nilai data yang tidak diketahui dengan menggunakan nilai data lain yang terkait dan diketahui.
"""

#Selanjutnya, untuk melatih data dengan KNN, tuliskan code berikut.
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, r2_score, mean_absolute_error, mean_squared_error

LR = LinearRegression()
LR.fit(X_train, y_train)
print ('Coefficients: ', LR .coef_)
print ('Intercept: ', LR .intercept_)
models.loc['train_mse','Linear Regression'] = mean_squared_error(y_pred = LR.predict(X_train), y_true=y_train)

"""Meskipun regresi linear sederhana, mudah dipahami, dan mudah digunakan, ia memiliki kelemahan karena hasil regresi merupakan hasil ramalan dari analisis regresi merupakan nilai estimasi sehingga kemungkinan untuk tidak sesuai dengan data aktual

## **2. Ridge Regression**

Regresi Ridge merupakan metode estimasi koefisien regresi yang diperoleh melalui penambahan konstanta bias sehingga diperoleh persamaan regresi linier yang baru dan tidak mengandung multikolinieritas
"""

from sklearn.linear_model import Ridge
RR = Ridge()
RR.fit(X_train, y_train)
models.loc['train_mse','Ridge Regression'] = mean_squared_error(y_pred = RR.predict(X_train), y_true=y_train)

"""## **3. Random Forest**

Algoritma random forest dapat digunakan untuk menyelesaikan masalah klasifikasi dan regresi. Random forest juga merupakan algoritma yang sering digunakan karena cukup sederhana tetapi memiliki stabilitas yang mumpuni.
"""

#Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor

#buat model prediksi
RF1 = RandomForestRegressor(random_state=100)
RF1.fit(X_train, y_train)
models.loc['train_mse','Random Forest'] = mean_squared_error(y_pred=RF1.predict(X_train), y_true=y_train)

"""## **4. Random Forest dengan Tuning GridSearchCV**

Untuk meningkatkan model, dilakukan eksperimen dengan menggunakan GridSearchCV untuk melakukan hyperparameter tuning pada Random Forest

Berikut adalah hyperparameter yang digunakan:

* n_estimator: jumlah trees (pohon) di forest
* max_depth: kedalaman atau panjang pohon. Ia merupakan ukuran seberapa banyak pohon dapat membelah (splitting) untuk membagi setiap node ke dalam jumlah pengamatan yang diinginkan.
* min_samples_split menentukan jumlah minimum sampel yang diperlukan untuk memisahkan simpul internal
* min_samples_leafmenentukan jumlah minimum sampel yang diperlukan untuk berada di simpul daun
"""

params = {'n_estimators' : [50,80,100],
          'max_depth' : [3,5,10],
           'min_samples_split':[2,3,4],
            'min_samples_leaf': [2,3,4]}

from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(estimator= RF1 , param_grid=params, cv=3, scoring='r2')
grid.fit(X_train, y_train)

grid.best_params_

RF2 = RandomForestRegressor(max_depth =10,min_samples_leaf = 4,min_samples_split = 2,n_estimators = 100,random_state=100)
RF2.fit(X_train, y_train)
models.loc['train_mse','Random Forest'] = mean_squared_error(y_pred=RF2.predict(X_train), y_true=y_train)

"""# **Evaluasi Model**

Sekarang, setelah model selesai dilatih dengan 4 algoritma, selanjutnya lihat performa model dengan menggunakan metrik evaluasi
"""

def metrics(name,key,arg):
    print('Name of the model: ',name)
    print('R^2 of the model:',r2_score(key,arg))
    print('MSE of the model:',np.sqrt(mean_squared_error(key,arg)))
    print('MAE of the model:',mean_absolute_error(key,arg))
    print('......')

y_LR = LR.predict(X_test)
y_RR = RR.predict(X_test)
y_RF1 = RF1.predict(X_test)
y_RF2 = RF2.predict(X_test)

metrics('Performa Model 1', y_test, y_LR)
metrics('Performa Model 2', y_test, y_RR)
metrics('Performa Model 3', y_test, y_RF1)
metrics('Performa Model 4', y_test, y_RF2)

"""Sekarang, setelah model selesai dilatih, lakukan proses scaling terhadap data uji. Hal ini harus dilakukan agar skala antara data latih dan data uji sama dan bisa dilakukan evaluasi.

Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
"""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""Selanjutnya, evaluasi ketiga model yang ada dengan metrik MSE yang telah dijelaskan di atas."""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['LR','RR','RF1', 'RF2'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'LR': LR, 'RR': RR, 'RF1': RF1, 'RF2': RF2}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

"""Untuk memudahkan, plot metrik tersebut dengan bar chart. Implementasikan kode di bawah ini:"""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Untuk mengujinya, buat prediksi menggunakan beberapa harga dari data test."""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Terlihat bahwa prediksi dengan Random Forest (RF), baik RF1 ataupun RF2 memberikan hasil yang paling mendekati y_true."""